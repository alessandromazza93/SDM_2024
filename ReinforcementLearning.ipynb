{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Se0+lRVt+NTez2Gk1fwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandromazza93/SDM_2024/blob/main/ReinforcementLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu64tVOt7_kL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize Python\n",
        "#------------------------------------------------\n",
        "#           Play around with these\n",
        "#------------------------------------------------\n",
        "alpha = 0.1  # Learning rate [0 = no influence of PEs, 1 = total influence]\n",
        "beta = 0.05  # Inverse temperature [0 = deterministic, Inf = random behavior]\n",
        "Qzero_A = 0.5  # Initial values for option A\n",
        "Qzero_B = 0.5  # Initial values for options B\n",
        "#------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set pre-defined outcomes\n",
        "nTrials = 18  # Number of trials\n",
        "reward_goes_to = np.array([\"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"A\", \"A\",\n",
        "                           \"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\"])\n",
        "# pRewA = 0.75\n",
        "# reward_goes_to = np.array([\"A\" if np.random.rand() <= pRewA else \"B\" for _ in range(nTrials)])\n",
        "\n",
        "# Simulate data\n",
        "Q = np.array([[Qzero_A, Qzero_B]])\n",
        "P = []\n",
        "choices = []\n",
        "rewards = []\n",
        "\n",
        "for t in range(nTrials):\n",
        "    # Compute choice probabilities [Softmax]\n",
        "    #--------------------------------------------\n",
        "    prob_A = np.exp(Q[t, 0] / beta) / (np.exp(Q[t, 0] / beta) + np.exp(Q[t, 1] / beta))  # P(A)\n",
        "    prob_B = np.exp(Q[t, 1] / beta) / (np.exp(Q[t, 0] / beta) + np.exp(Q[t, 1] / beta))  # P(B)\n",
        "    P.append([prob_A, prob_B])\n",
        "\n",
        "    # Simulate choice based on probabilities\n",
        "    #---------------------------------------\n",
        "    if np.random.rand() < prob_A:\n",
        "        choices.append(\"A\")  # Choose option A\n",
        "    else:\n",
        "        choices.append(\"B\")  # Choose option B\n",
        "\n",
        "    # Get outcome for the chosen option\n",
        "    #------------------------------------\n",
        "    if choices[-1] == reward_goes_to[t]:\n",
        "        rewards.append(1)\n",
        "    else:\n",
        "        rewards.append(0)\n",
        "\n",
        "    # Update values [Rescorla-Wagner rule]\n",
        "    #------------------------------------------\n",
        "    if choices[-1] == \"A\":\n",
        "        Q_t1_A = Q[t, 0] + alpha * (rewards[-1] - Q[t, 0])  # Update value for option A\n",
        "        Q_t1_B = Q[t, 1]  # Keep same value for option B\n",
        "    elif choices[-1] == \"B\":\n",
        "        Q_t1_B = Q[t, 1] + alpha * (rewards[-1] - Q[t, 1])  # Update value for option B\n",
        "        Q_t1_A = Q[t, 0]  # Keep same value for option A\n",
        "\n",
        "    Q = np.vstack([Q, [Q_t1_A, Q_t1_B]])  # Append updated values\n",
        "\n",
        "# Cut n+1th trial\n",
        "Q = Q[:-1, :]\n",
        "P = np.array(P)\n",
        "\n",
        "# Plot values over time\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(211)\n",
        "plt.plot(range(1, nTrials + 1), Q[:, 0], '-o', linewidth=2, label='Q(A)')\n",
        "plt.plot(range(1, nTrials + 1), Q[:, 1], '-o', linewidth=2, label='Q(B)')\n",
        "plt.xlabel('Trial')\n",
        "plt.ylabel('Subjective value')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Choice Values')\n",
        "plt.ylim([0, 1])\n",
        "\n",
        "# Plot probabilities over time\n",
        "plt.subplot(212)\n",
        "plt.plot(range(1, nTrials + 1), P[:, 0], '-o', linewidth=2, label='P(A)')\n",
        "plt.plot(range(1, nTrials + 1), P[:, 1], '-o', linewidth=2, label='P(B)')\n",
        "plt.xlabel('Trial')\n",
        "plt.ylabel('Probability')\n",
        "plt.legend(loc='upper left')\n",
        "plt.axhline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
        "for i, choice in enumerate(choices):\n",
        "    plt.axvline(x=i + 1, color='blue' if choice == \"A\" else 'red', linestyle='--', alpha=0.5)\n",
        "\n",
        "reward_indices = np.array(rewards) == 1\n",
        "non_reward_indices = np.array(rewards) == 0\n",
        "plt.scatter(np.where(reward_indices)[0] + 1, 1.1 * np.ones(np.sum(reward_indices)), color='green', label='Reward', zorder=5)\n",
        "plt.scatter(np.where(non_reward_indices)[0] + 1, 1.1 * np.ones(np.sum(non_reward_indices)), color='red', label='No Reward', zorder=5)\n",
        "plt.title('Choice Probabilities')\n",
        "plt.ylim([-0.1, 1.1])\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}